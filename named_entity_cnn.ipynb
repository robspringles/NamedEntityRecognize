{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet.contrib import text\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import open\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<PAD>'\n",
    "NOT = 'N'\n",
    "PAD_NATURE = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "drop_prob = 0.2\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "\n",
    "max_seq_len = 30\n",
    "\n",
    "word_vec_size = 200\n",
    "nature_vec_size = 50\n",
    "distance_vec_size = 50\n",
    "num_channels = 10\n",
    "conv_width = word_vec_size + nature_vec_size + distance_vec_size\n",
    "kernels_size_ls = [(2, conv_width), (3, conv_width), (4, conv_width)]\n",
    "padding_ls = None\n",
    "pool_size = (2, 1) \n",
    "output_size = 6\n",
    "distance_size = 2 * max_seq_len - 1\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(max_seq_len):\n",
    "    input_tokens = []   # 记录输入 X 的所有词，包含重复\n",
    "    output_tokens = []  # 记录输出 Y 的所有符号，包含重复\n",
    "    nature_tokens = []  # 记录所有词的词性的符号，包含重复\n",
    "    input_seqs = []  # 列表中装的列表，里面的每个列表代表一条输入，填充或截断好了的\n",
    "    output_seqs = []  # 同input_seqs\n",
    "    nature_seqs = []\n",
    "    \n",
    "    with open(\"../data_for_seq2seq/re_cut_lines_word.txt\", 'r') as fx, open(\"../data_for_seq2seq/re_cut_lines_label.txt\", 'r') as fy, open(\"../data_for_seq2seq/re_cut_lines_nature.txt\", 'r') as fn:\n",
    "        word_lines = fx.readlines()\n",
    "        label_lines = fy.readlines()\n",
    "        word_natures = fn.readlines()\n",
    "        \n",
    "        for word_line, lable_line, word_nature in zip(word_lines, label_lines, word_natures):\n",
    "            \n",
    "            input_seq = word_line.strip()\n",
    "            output_seq = lable_line.strip()\n",
    "            nature_seq = word_nature.strip()\n",
    "            \n",
    "            cur_input_tokens = input_seq.split(' ')\n",
    "            cur_output_tokens = output_seq.split(' ')\n",
    "            cur_nature_tokens = nature_seq.split(' ')\n",
    "            \n",
    "            if '' in cur_output_tokens:\n",
    "                continue\n",
    "            \n",
    "            if len(cur_input_tokens) < max_seq_len or len(cur_output_tokens) < max_seq_len or len(cur_nature_tokens) < max_seq_len:\n",
    "                input_tokens.extend(cur_input_tokens)\n",
    "                output_tokens.extend(cur_output_tokens)\n",
    "                nature_tokens.extend(cur_nature_tokens)\n",
    "                \n",
    "                # 添加 PAD 符号使每个序列等长，长度为 max_seq_len\n",
    "                while len(cur_input_tokens) < max_seq_len:\n",
    "                    cur_input_tokens.append(PAD)\n",
    "                    # 把输出也填充到了最大长度\n",
    "                    cur_output_tokens.append(NOT)\n",
    "                    cur_nature_tokens.append(PAD_NATURE)\n",
    "                    \n",
    "                input_seqs.append(cur_input_tokens)                            \n",
    "                output_seqs.append(cur_output_tokens)\n",
    "                nature_seqs.append(cur_nature_tokens)\n",
    "                \n",
    "            else:\n",
    "                cur_input_tokens = cur_input_tokens[0: max_seq_len]\n",
    "                cur_output_tokens = cur_output_tokens[0: max_seq_len]\n",
    "                cur_nature_tokens = cur_nature_tokens[0: max_seq_len]\n",
    "                \n",
    "                input_tokens.extend(cur_input_tokens)\n",
    "                input_seqs.append(cur_input_tokens)\n",
    "                \n",
    "                output_tokens.extend(cur_output_tokens)\n",
    "                output_seqs.append(cur_output_tokens)\n",
    "                \n",
    "                nature_tokens.extend(cur_nature_tokens)\n",
    "                nature_seqs.append(cur_nature_tokens)\n",
    "                \n",
    "        fr_vocab = text.vocab.Vocabulary(collections.Counter(input_tokens), reserved_tokens=[PAD])\n",
    "        print(collections.Counter(output_tokens))\n",
    "        en_vocab = text.vocab.Vocabulary(collections.Counter(output_tokens))\n",
    "        \n",
    "        nature_vocab = text.vocab.Vocabulary(collections.Counter(nature_tokens))\n",
    "    \n",
    "    return fr_vocab, en_vocab, nature_vocab, input_seqs, output_seqs, nature_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab, output_vocab, nature_vocab, input_seqs, output_seqs, nature_seqs = read_data(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../data_for_cnn_lstm/X.npy\") and  os.path.exists(\"../data_for_cnn_lstm/Y.npy\") and os.path.exists(\"../data_for_cnn_lstm/nature.npy\"):\n",
    "    print(\"Loading...\")\n",
    "    X = np.load(\"../data_for_cnn_lstm/X.npy\")\n",
    "    Y = np.load(\"../data_for_cnn_lstm/Y.npy\")\n",
    "    nature = np.load(\"../data_for_cnn_lstm/nature.npy\")\n",
    "    print(\"End\")\n",
    "else:\n",
    "    print(\"Converting...\")\n",
    "    X = nd.zeros((len(input_seqs), max_seq_len))\n",
    "    Y = nd.zeros((len(output_seqs), max_seq_len))\n",
    "    nature = nd.zeros((len(nature_seqs), max_seq_len))\n",
    "    \n",
    "    for i in range(len(input_seqs)):\n",
    "        X[i] = nd.array(input_vocab.to_indices(input_seqs[i]))\n",
    "        Y[i] = nd.array(output_vocab.to_indices(output_seqs[i]))\n",
    "        nature[i] = nd.array(nature_vocab.to_indices(nature_seqs[i]))\n",
    "    np.save(\"../data_for_cnn_lstm/X.npy\", X.asnumpy())\n",
    "    np.save(\"../data_for_cnn_lstm/Y.npy\", Y.asnumpy())\n",
    "    np.save(\"../data_for_cnn_lstm/nature.npy\", nature.asnumpy())\n",
    "    print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nature.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, nature_train, nature_test = train_test_split(X, Y, nature, test_size=0.1, random_state=33)\n",
    "((X_train.shape, Y_train.shape, nature_train.shape), (X_test.shape, Y_test.shape, nature_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = gluon.data.ArrayDataset(nd.array(X_train, ctx=ctx), nd.array(Y_train, ctx=ctx), nd.array(nature_train, ctx=ctx))\n",
    "data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, Y_train, nature_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Block):\n",
    "    def __init__(self, vocab_size, word_vec_size, nature_size, nature_vec_size, distance_size, distance_vec_size,\n",
    "                 num_channels, kernels_size_ls, padding_ls, pool_size, output_size,\n",
    "                 drop_prob=0.2,  **kwargs):\n",
    "        super(CNN_Model, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.word_embedding = nn.Embedding(vocab_size, word_vec_size)\n",
    "            self.nature_embedding = nn.Embedding(nature_size, nature_vec_size)\n",
    "            self.distance_embedding = nn.Embedding(distance_size, distance_vec_size)\n",
    "            self.num_channels = num_channels\n",
    "            self.kernels_size_ls = kernels_size_ls\n",
    "            self.conv_ls = []\n",
    "            for kernel_size in kernels_size_ls:\n",
    "                self.conv_ls.append(nn.Conv2D(channels=num_channels, kernel_size=kernel_size, activation='relu',\n",
    "                                             weight_initializer=\"normal\"))\n",
    "            self.max_pool = nn.MaxPool2D(pool_size=pool_size)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.dense = nn.Dense(output_size)\n",
    "            self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x_input, nature_input, distance_input):\n",
    "        batch_words_embed = self.word_embedding(x_input)\n",
    "        batch_nature_embed = self.nature_embedding(nature_input)\n",
    "        batch_distance_embed = self.distance_embedding(distance_input)\n",
    "        \n",
    "        # (batch_size, height, width)\n",
    "        print(batch_words_embed.shape)\n",
    "        print(batch_nature_embed.shape)\n",
    "        print(batch_distance_embed.shape)\n",
    "        batch_data_x = nd.concat(batch_words_embed, batch_nature_embed, batch_distance_embed, dim=2)\n",
    "        # (batch_size, 1, height, width)\n",
    "        batch_data_x = nd.expand_dims(batch_data_x, axis=1)\n",
    "        \n",
    "        conv_pool_result = []\n",
    "        for conv in self.conv_ls:\n",
    "            conv_result = conv(batch_data_x)    # (batch_size, num_channels, out_height, out_width)\n",
    "            pool_result = self.max_pool(conv_result)    # (batch_size, num_channels, new_height, new_width)\n",
    "            pool_result = self.flatten(pool_result)\n",
    "            conv_pool_result.append(pool_result)\n",
    "        # (batch_size, len(kernel_size_ls)*num_channels*new_height,new_width)\n",
    "        conv_pool_result_concated = nd.concat(*conv_pool_result, dim=1)\n",
    "        conv_pool_result_concated = self.drop(conv_pool_result_concated)\n",
    "        output = self.dense(conv_pool_result_concated)\n",
    "        \n",
    "        return output   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_value = nd.array(list(output_vocab.token_to_idx.values()), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_one_hot = nd.one_hot(dic_value, dic_value.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cnn_input(word_data, nature, batch_distance, pos):\n",
    "    x_input = word_data[:, pos]\n",
    "    nature_input = nature[:, pos]\n",
    "    distance_input = batch_distance[:, pos]\n",
    "    \n",
    "    return x_input, nature_input, distance_input   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, max_seq_len, label_one_hot, output_vocab, learning_rate, ctx):\n",
    "    # 对于三个网络，分别初始化它们的模型参数并定义它们的优化器。\n",
    "    model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    \n",
    "    optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                                      {'learning_rate': learning_rate})\n",
    "\n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    total_loss = []   \n",
    "    for epoch in range(0, epochs):\n",
    "        epoch_loss = 0.0\n",
    "        batch_idx = 0\n",
    "        for x, y, nature in data_iter_train:\n",
    "            batch_preds = []\n",
    "            with autograd.record():\n",
    "                batch_loss = nd.array([0], ctx=ctx)\n",
    "                \n",
    "                for word_idx in range(x.shape[1]): \n",
    "                    distance = nd.arange(x.shape[1], ctx=ctx) - word_idx\n",
    "                    distance = distance.reshape((1, -1))\n",
    "                    # batch_distance 尺寸: (batch_size, max_seq_length)\n",
    "                    batch_distance = nd.broadcast_axis(distance, axis=0, size=batch_size)                    \n",
    "                    \n",
    "                    outputs = model(x, nature, batch_distance)\n",
    "                    preds = nd.argmax(nd.softmax(outputs, axis=1), axis=1)\n",
    "                    print(pred.shape)\n",
    "                    batch_preds.append(preds)\n",
    "                    y_idx = y[:, word_idx]\n",
    "                    label = nd.take(label_one_hot, y_idx)\n",
    "                    \n",
    "                    batch_loss = batch_loss + nd.mean(softmax_cross_entropy(outputs, label))\n",
    "                \n",
    "            batch_loss.backward()\n",
    "            optimizer.step(batch_size)\n",
    "        \n",
    "            epoch_loss += batch_loss.asscalar()                   \n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\"epoch: {0} , batch: {1}, batch_loss: {2}\".format(epoch, batch_idx, batch_loss.asscalar()))\n",
    "#                 for idx in range(2): \n",
    "#                     true_idx = [int(x) for x in list(y[idx].asnumpy())]\n",
    "#                     pred_idx = [int(x) for x in list(pred_outputs[idx].asnumpy())]\n",
    "                    \n",
    "#                     true_label = output_vocab.to_tokens(true_idx)\n",
    "#                     pred_label = output_vocab.to_tokens(pred_idx)\n",
    "                    \n",
    "#                     print(\"Sapmle {0} :\".format(idx))\n",
    "#                     print(\"True label : {0}\".format(true_label))\n",
    "#                     print(\"Pred label : {0}\".format(pred_label))\n",
    "            batch_idx += 1\n",
    "            \n",
    "        \n",
    "        epoch_loss = epoch_loss / batch_idx\n",
    "        \n",
    "        \n",
    "        total_loss.append(epoch_loss)\n",
    "        \n",
    "        print(\"epoch: {0} , epoch_loss: {1}\".format(epoch, epoch_loss))\n",
    "        print(\"-----------------------------------------------------\")\n",
    "    \n",
    "    plt.plot(range(epochs), total_loss)\n",
    "    plt.show()\n",
    "    \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model(len(input_vocab), word_vec_size, len(nature_vocab), nature_vec_size, distance_size, distance_vec_size,\n",
    "                 num_channels, kernels_size_ls, padding_ls, pool_size, output_size, drop_prob=drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, max_seq_len, label_one_hot, output_vocab, learning_rate, ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
